{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66dd7068-3882-48eb-b908-fa2c525d6d90",
   "metadata": {},
   "source": [
    "# TEP-NET Model Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dfcb31-8c36-4b8a-9070-dc5eff851e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, MultiHeadAttention, Dense, GlobalAveragePooling1D, Dropout, Concatenate, Layer, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "from tensorflow.keras.models import load_model\n",
    "import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import h5py\n",
    "import sys\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from lime.lime_tabular import LimeTabularExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc851430-41e8-435d-abe8-8786665f3eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ''\n",
    "\n",
    "def load_hdf5_in_chunks(h5_path, chunk_size=5000):\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        total_samples = f['TCR'].shape[0]\n",
    "        meta_keys = list(f['meta'].keys())\n",
    "\n",
    "        for i in range(0, total_samples, chunk_size):\n",
    "            end = min(i + chunk_size, total_samples)\n",
    "\n",
    "            TCR_chunk = f['TCR'][i:end]\n",
    "            epitope_chunk = f['epitope'][i:end]\n",
    "\n",
    "            meta_chunk = {\n",
    "                key: f[f'meta/{key}'][i:end] for key in meta_keys\n",
    "            }\n",
    "\n",
    "            df_chunk = pd.DataFrame(meta_chunk)\n",
    "            df_chunk['TCR'] = list(TCR_chunk)\n",
    "            df_chunk['epitope'] = list(epitope_chunk)\n",
    "\n",
    "            yield df_chunk\n",
    "\n",
    "def load_data(filename):\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for df_chunk in load_hdf5_in_chunks(f'{path}{filename}', chunk_size=5000):\n",
    "        df = pd.concat([df, df_chunk], sort=False)  \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bf1425-555d-4161-9996-b332bf7ecf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # Create a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Extract feature columns and labels\n",
    "    X_features = df[FEATURE_COLUMNS].values\n",
    "    y_labels = df['binding'].values\n",
    "\n",
    "    # Convert all to TensorFlow tensors\n",
    "    X_tcr = np.stack(df['TCR'].values)\n",
    "    X_epitope = np.stack(df['epitope'].values)\n",
    "    X_features = tf.convert_to_tensor(X_features, dtype=tf.float32)\n",
    "    y_labels = tf.convert_to_tensor(y_labels, dtype=tf.float32)\n",
    "    \n",
    "    return X_tcr, X_epitope, X_features, y_labels\n",
    "\n",
    "def preprocess_tpp(df):\n",
    "    # Create a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "       \n",
    "    # Extract feature columns and labels\n",
    "    X_features = df[FEATURE_COLUMNS].values\n",
    "    y_labels = df['binding'].values\n",
    "\n",
    "    # Convert all to TensorFlow tensors\n",
    "    X_tcr = np.stack(df['TCR'].values)\n",
    "    X_epitope = np.stack(df['epitope'].values)\n",
    "    X_features = tf.convert_to_tensor(X_features, dtype=tf.float32)\n",
    "    y_labels = tf.convert_to_tensor(y_labels, dtype=tf.float32)\n",
    "    \n",
    "    return X_tcr, X_epitope, X_features, y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dbe6fd-9ca3-426b-9b2f-bacd2f0d43ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 50000\n",
    "EMBEDDING = 64\n",
    "EMBEDDING_TYPE = 'pca'\n",
    "FEATURE_COLUMNS = ['TCR_KF7', 'TCR_KF1', 'TCR_hydrophobicity', 'TCR_aromaticity', \n",
    "                   'TCR_isoelectric_point', 'TCR_instability_index', \n",
    "                   'epitope_KF7', 'epitope_KF1','epitope_hydrophobicity', 'epitope_aromaticity',\n",
    "                   'epitope_isoelectric_point', 'epitope_instability_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e1cb6-460a-44cd-b608-c27b062773f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = load_data(f'test_ProtBERT_{EMBEDDING}_{EMBEDDING_TYPE}.h5')\n",
    "X_test_tcr, X_test_epitope, X_test_features, y_test_labels = preprocess_data(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebfe7d5-d9ee-40a4-b840-d0d10c37200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate binding and non-binding samples\n",
    "df_negative = df_test[df_test['binding'] == 0]\n",
    "df_positive = df_test[df_test['binding'] == 1]\n",
    "\n",
    "X_positive_tcr, X_positive_epitope, X_positive_features, y_positive_labels = preprocess_data(df_positive)\n",
    "X_negative_tcr, X_negative_epitope, X_negative_features, y_negative_labels = preprocess_data(df_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8fc4f8-f230-4a14-a14e-bcb5fbc2a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = load_data(f'test_ProtBERT_{EMBEDDING}_{EMBEDDING_TYPE}.h5')\n",
    "X_test_tcr, X_test_epitope, X_test_features, y_test_labels = preprocess_data(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadec521-8327-4551-a10c-bb2ffad2251c",
   "metadata": {},
   "source": [
    "## Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2082ab-2504-42d6-9f92-089fb1266e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "\n",
    "@register_keras_serializable()\n",
    "def f1_score_metric(y_true, y_pred):\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "    tp = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32))\n",
    "    fp = tf.reduce_sum(tf.cast((1 - y_true) * y_pred, tf.float32))\n",
    "    fn = tf.reduce_sum(tf.cast(y_true * (1 - y_pred), tf.float32))s\n",
    "    \n",
    "    precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
    "    recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "    return f1\n",
    "\n",
    "@register_keras_serializable()\n",
    "class ExpandDimsLayer(Layer):\n",
    "    def __init__(self, axis=1, **kwargs):\n",
    "        super(ExpandDimsLayer, self).__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.expand_dims(inputs, axis=self.axis)\n",
    "\n",
    "    def get_config(self):\n",
    "        # Add the configuration parameters\n",
    "        config = super(ExpandDimsLayer, self).get_config()\n",
    "        config.update({\n",
    "            \"axis\": self.axis\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@register_keras_serializable()\n",
    "class PiecewiseLinearEncoding(Layer):\n",
    "    def __init__(self, bins, **kwargs):\n",
    "        super(PiecewiseLinearEncoding, self).__init__(**kwargs)\n",
    "        self.bins = tf.convert_to_tensor(bins, dtype=tf.float32)\n",
    "        self.num_bins = len(bins) - 1\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Expand input to shape [batch_size, num_features, 1]\n",
    "        inputs_expanded = tf.expand_dims(inputs, axis=-1)\n",
    "        \n",
    "        # Compute the widths of bins\n",
    "        bin_widths = self.bins[1:] - self.bins[:-1]\n",
    "\n",
    "        # Compute piecewise linear encoding\n",
    "        bin_edges = (inputs_expanded - self.bins[:-1]) / bin_widths\n",
    "        bin_edges = tf.clip_by_value(bin_edges, 0.0, 1.0)\n",
    "\n",
    "        return bin_edges\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PiecewiseLinearEncoding, self).get_config()\n",
    "        config.update({\n",
    "            \"bins\": self.bins.numpy().tolist()\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@register_keras_serializable()\n",
    "class PeriodicEmbeddings(Layer):\n",
    "    def __init__(self, num_frequencies=16, **kwargs):\n",
    "        super(PeriodicEmbeddings, self).__init__(**kwargs)\n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.freqs = tf.Variable(\n",
    "            initial_value=tf.random.uniform(\n",
    "                shape=(num_frequencies,), minval=0.1, maxval=1.0\n",
    "            ),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Shape of inputs: [batch_size, num_features]\n",
    "        inputs_expanded = tf.expand_dims(inputs, axis=-1)  # [batch_size, num_features, 1]\n",
    "        periodic_features = tf.concat(\n",
    "            [\n",
    "                tf.sin(2 * np.pi * inputs_expanded * self.freqs),\n",
    "                tf.cos(2 * np.pi * inputs_expanded * self.freqs),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "\n",
    "        return periodic_features\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PeriodicEmbeddings, self).get_config()\n",
    "        config.update({\n",
    "            \"num_frequencies\": self.num_frequencies\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f100fb16-7e16-4607-83c9-4e049e9c3b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = ''\n",
    "model = keras.saving.load_model(f\"{path_model}V3_model_1-5_64_pca.keras\",\n",
    "                                custom_objects={\"ExpandDimsLayer\": ExpandDimsLayer, \"PeriodicEmbeddings\": PeriodicEmbeddings, \"PiecewiseLinearEncoding\": PiecewiseLinearEncoding, \"f1_score_metric\": f1_score_metric},\n",
    "                                safe_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c19f2-8235-4e2d-8af1-fd4976f2d4bf",
   "metadata": {},
   "source": [
    "# Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe5eea6-7753-4181-a76e-fe1890b190c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import numpy as np\n",
    "\n",
    "feature_names = FEATURE_COLUMNS\n",
    "class_names = ['non-binding', 'binding']\n",
    "\n",
    "# Convert tensors to NumPy arrays\n",
    "X_test_features_np = X_test_features.numpy()\n",
    "y_test_np = y_test_labels.numpy()\n",
    "\n",
    "explainer = LimeTabularExplainer(\n",
    "    training_data=X_test_features_np,\n",
    "    feature_names=feature_names,\n",
    "    class_names=class_names,\n",
    "    mode='classification',\n",
    "    discretize_continuous=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad61bf6-f119-439d-84dd-350e5c516c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_predict_fn(physicochemical_input_np):\n",
    "    dummy_tcr = np.mean(X_test_tcr, axis=0, keepdims=True)\n",
    "    dummy_epitope = np.mean(X_test_epitope, axis=0, keepdims=True)\n",
    "\n",
    "    dummy_tcr_batch = np.repeat(dummy_tcr, len(physicochemical_input_np), axis=0)\n",
    "    dummy_epitope_batch = np.repeat(dummy_epitope, len(physicochemical_input_np), axis=0)\n",
    "\n",
    "    preds = model.predict({\n",
    "        \"TCR_Input\": dummy_tcr_batch,\n",
    "        \"Epitope_Input\": dummy_epitope_batch,\n",
    "        \"Physicochemical_Features\": tf.convert_to_tensor(physicochemical_input_np, dtype=tf.float32)\n",
    "    }, verbose=0)\n",
    "\n",
    "    return np.hstack([1 - preds, preds])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c4b702-c87b-475b-8ae9-b2aa8797ab6a",
   "metadata": {},
   "source": [
    "## Check one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62068ef0-53fa-425f-a2f2-d76031ff9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 44\n",
    "exp = explainer.explain_instance(\n",
    "    data_row=X_test_features_np[sample_idx],\n",
    "    predict_fn=lime_predict_fn,\n",
    "    num_features=10\n",
    ")\n",
    "\n",
    "exp.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76073b6-747a-4465-9684-f50962647978",
   "metadata": {},
   "source": [
    "## Check multiple sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59855b05-dc0d-440d-ab97-70f1f193c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def aggregate_lime_explanations(\n",
    "    X_features_np,\n",
    "    num_samples,\n",
    "    num_features=10,\n",
    "    random_seed=42\n",
    "):\n",
    "    np.random.seed(random_seed)\n",
    "    selected_indices = np.random.choice(len(X_features_np), size=num_samples, replace=False)\n",
    "\n",
    "    aggregate_weights = collections.defaultdict(float)\n",
    "    feature_counts = collections.defaultdict(int)\n",
    "\n",
    "    for idx in selected_indices:\n",
    "        exp = explainer.explain_instance(\n",
    "            data_row=X_features_np[idx],\n",
    "            predict_fn=lime_predict_fn,\n",
    "            num_features=num_features\n",
    "        )\n",
    "        \n",
    "        for feature, weight in exp.as_list():\n",
    "            aggregate_weights[feature] += weight\n",
    "            feature_counts[feature] += 1\n",
    "\n",
    "    aggregated = {\n",
    "        feature: aggregate_weights[feature] / feature_counts[feature]\n",
    "        for feature in aggregate_weights\n",
    "    }\n",
    "\n",
    "    df_lime = pd.DataFrame(\n",
    "        list(aggregated.items()),\n",
    "        columns=[\"Feature (Condition)\", \"Mean LIME Weight\"]\n",
    "    ).sort_values(by=\"Mean LIME Weight\", ascending=False)\n",
    "\n",
    "    return df_lime\n",
    "\n",
    "\n",
    "df_lime = aggregate_lime_explanations(X_test_features_np, NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671ee616-1cb7-454b-926c-6f0620b3f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.barh(df_lime[\"Feature (Condition)\"], df_lime[\"Mean LIME Weight\"])\n",
    "plt.gca().invert_yaxis()\n",
    "#plt.title(\"Aggregated LIME Feature Importances (Top Conditions)\")\n",
    "plt.xlabel(\"Mean LIME Weight\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{path}aggregated_lime_feature_importance_{NUM_SAMPLES}.png', dpi=96, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1387ad3e-21c6-4533-982e-87ca9503c8d5",
   "metadata": {},
   "source": [
    "### Check binding vs non binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20819288-5be4-429f-8e0e-2425a658b0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_idx = np.where(y_val_np == 1)[0][0]\n",
    "neg_idx = np.where(y_val_np == 0)[0][0]\n",
    "\n",
    "data_pos = X_test_features[pos_idx].numpy() if tf.is_tensor(X_test_features[pos_idx]) else X_test_features[pos_idx]\n",
    "data_neg = X_test_features[neg_idx].numpy() if tf.is_tensor(X_test_features[neg_idx]) else X_test_features[neg_idx]\n",
    "\n",
    "exp_pos = explainer.explain_instance(\n",
    "    data_row=data_pos,\n",
    "    predict_fn=lime_predict_fn,\n",
    "    num_features=10\n",
    ")\n",
    "\n",
    "exp_neg = explainer.explain_instance(\n",
    "    data_row=data_neg,\n",
    "    predict_fn=lime_predict_fn,\n",
    "    num_features=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff18508-4f70-44d8-90b2-3363707979ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_val_np = y_test_labels.numpy() if tf.is_tensor(y_test_labels) else y_test_labels\n",
    "\n",
    "pos_idx = np.where(y_val_np == 1)[0][0]\n",
    "neg_idx = np.where(y_val_np == 0)[0][0]\n",
    "\n",
    "data_pos = X_test_features[pos_idx].numpy() if tf.is_tensor(X_test_features[pos_idx]) else X_test_features[pos_idx]\n",
    "data_neg = X_test_features[neg_idx].numpy() if tf.is_tensor(X_test_features[neg_idx]) else X_test_features[neg_idx]\n",
    "\n",
    "exp_pos = explainer.explain_instance(data_row=data_pos, predict_fn=lime_predict_fn, num_features=10)\n",
    "exp_neg = explainer.explain_instance(data_row=data_neg, predict_fn=lime_predict_fn, num_features=10)\n",
    "\n",
    "exp_pos.show_in_notebook(show_table=True)\n",
    "exp_neg.show_in_notebook(show_table=True)\n",
    "\n",
    "fig1 = exp_pos.as_pyplot_figure(label=1)\n",
    "fig1.suptitle(\"Binding Sample\", fontsize=14)\n",
    "\n",
    "fig2 = exp_neg.as_pyplot_figure(label=1)\n",
    "fig2.suptitle(\"Non-binding Sample\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23566e2d-a254-4b07-a5ff-026e4c50cdff",
   "metadata": {},
   "source": [
    "### Gradient × Input (Saliency Map-like) for TCR & Epitope Embeddings\n",
    "You can compute the gradient of the model output w.r.t. the input embeddings to see which parts of the input embeddings were most influential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903309f2-24a9-4de9-b1b7-2c45ab9b3a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def compute_input_gradients(model, tcr_input, epitope_input, feature_input):\n",
    "    tcr_input = tf.convert_to_tensor(tcr_input[None, ...])\n",
    "    epitope_input = tf.convert_to_tensor(epitope_input[None, ...])\n",
    "    feature_input = tf.convert_to_tensor(feature_input[None, ...])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch([tcr_input, epitope_input])\n",
    "        output = model({\n",
    "            \"TCR_Input\": tcr_input,\n",
    "            \"Epitope_Input\": epitope_input,\n",
    "            \"Physicochemical_Features\": feature_input\n",
    "        })\n",
    "    \n",
    "    # Gradients of output w.r.t. inputs\n",
    "    grad_tcr, grad_epitope = tape.gradient(output, [tcr_input, epitope_input])\n",
    "\n",
    "    # Saliency = abs(gradient × input)\n",
    "    saliency_tcr = tf.reduce_sum(tf.abs(grad_tcr * tcr_input), axis=-1).numpy()[0]\n",
    "    saliency_epitope = tf.reduce_sum(tf.abs(grad_epitope * epitope_input), axis=-1).numpy()[0]\n",
    "\n",
    "    return saliency_tcr, saliency_epitope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec64525-c570-49b5-8554-2ad6aeef3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "instance_idx = 0\n",
    "\n",
    "sal_tcr, sal_epi = compute_input_gradients(\n",
    "    model,\n",
    "    tcr_input=X_test_tcr[instance_idx],\n",
    "    epitope_input=X_test_epitope[instance_idx],\n",
    "    feature_input=X_test_features[instance_idx]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.bar(range(26), sal_tcr)\n",
    "plt.title(\"TCR Embedding Saliency (Gradient × Input)\")\n",
    "plt.xlabel(\"TCR Position\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.bar(range(24), sal_epi)\n",
    "plt.title(\"Epitope Embedding Saliency (Gradient × Input)\")\n",
    "plt.xlabel(\"Epitope Position\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c897f90-f915-4847-b506-5528cecf86c0",
   "metadata": {},
   "source": [
    "### Multiple Saliency Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08218866-6426-4476-b096-13c229ca9b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_saliency(model, tcr_batch, epitope_batch, feature_batch):\n",
    "    tcr_batch = tf.convert_to_tensor(tcr_batch, dtype=tf.float32)\n",
    "    epitope_batch = tf.convert_to_tensor(epitope_batch, dtype=tf.float32)\n",
    "    feature_batch = tf.convert_to_tensor(feature_batch, dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch([tcr_batch, epitope_batch])\n",
    "        outputs = model({\n",
    "            \"TCR_Input\": tcr_batch,\n",
    "            \"Epitope_Input\": epitope_batch,\n",
    "            \"Physicochemical_Features\": feature_batch\n",
    "        })  # shape: (batch_size, 1)\n",
    "\n",
    "    grad_tcr, grad_epitope = tape.gradient(outputs, [tcr_batch, epitope_batch])\n",
    "\n",
    "    # Compute saliency: |gradient × input|, then sum over embedding dim\n",
    "    saliency_tcr = tf.reduce_sum(tf.abs(grad_tcr * tcr_batch), axis=-1).numpy()\n",
    "    saliency_epi = tf.reduce_sum(tf.abs(grad_epitope * epitope_batch), axis=-1).numpy()\n",
    "\n",
    "    return saliency_tcr, saliency_epi\n",
    "\n",
    "def compute_saliency_over_dataset(model, X_tcr, X_epi, X_feat, batch_size=128):\n",
    "    total_saliency_tcr = []\n",
    "    total_saliency_epi = []\n",
    "\n",
    "    num_samples = X_tcr.shape[0]\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        tcr_batch = X_tcr[i:i + batch_size]\n",
    "        epi_batch = X_epi[i:i + batch_size]\n",
    "        feat_batch = X_feat[i:i + batch_size]\n",
    "\n",
    "        sal_tcr, sal_epi = compute_batch_saliency(model, tcr_batch, epi_batch, feat_batch)\n",
    "        total_saliency_tcr.append(sal_tcr)\n",
    "        total_saliency_epi.append(sal_epi)\n",
    "\n",
    "    all_saliency_tcr = np.vstack(total_saliency_tcr)\n",
    "    all_saliency_epi = np.vstack(total_saliency_epi)\n",
    "\n",
    "    mean_saliency_tcr = np.mean(all_saliency_tcr, axis=0)\n",
    "    mean_saliency_epi = np.mean(all_saliency_epi, axis=0)\n",
    "\n",
    "    return mean_saliency_tcr, mean_saliency_epi\n",
    "\n",
    "\n",
    "n_samples = NUM_SAMPLES\n",
    "X_tcr_batch = X_test_tcr[:n_samples]\n",
    "X_epi_batch = X_test_epitope[:n_samples]\n",
    "X_feat_batch = X_test_features[:n_samples]\n",
    "\n",
    "mean_sal_tcr, mean_sal_epi = compute_saliency_over_dataset(model, X_tcr_batch, X_epi_batch, X_feat_batch, batch_size=128)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.bar(range(len(mean_sal_tcr)), mean_sal_tcr)\n",
    "#plt.title(f\"Average TCR Embedding Saliency (n={n_samples})\")\n",
    "plt.xlabel(\"TCR Position\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{path}average_tcr_embedding_saliency_{NUM_SAMPLES}.png', dpi=96, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.bar(range(len(mean_sal_epi)), mean_sal_epi)\n",
    "#plt.title(f\"Average Epitope Embedding Saliency (n={n_samples})\")\n",
    "plt.xlabel(\"Epitope Position\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{path}average_epitope_embedding_saliency_{NUM_SAMPLES}.png', dpi=96, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9965f8-5162-4a78-8a15-b394dac6e947",
   "metadata": {},
   "source": [
    "### Separate saliency by predicted label (binding vs. non-binding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c252a110-f86a-45c9-b6a1-2c10f10e6f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_prediction(model, X_tcr, X_epitope, X_features, threshold=0.5, batch_size=128):\n",
    "    preds = []\n",
    "    tcr_chunks = []\n",
    "    epi_chunks = []\n",
    "    feat_chunks = []\n",
    "\n",
    "    num_samples = X_tcr.shape[0]\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        tcr_batch = X_tcr[i:i + batch_size]\n",
    "        epi_batch = X_epitope[i:i + batch_size]\n",
    "        feat_batch = X_features[i:i + batch_size]\n",
    "\n",
    "        pred_batch = model.predict({\n",
    "            \"TCR_Input\": tcr_batch,\n",
    "            \"Epitope_Input\": epi_batch,\n",
    "            \"Physicochemical_Features\": feat_batch\n",
    "        }, verbose=0).reshape(-1)\n",
    "\n",
    "        preds.append(pred_batch)\n",
    "        tcr_chunks.append(tcr_batch)\n",
    "        epi_chunks.append(epi_batch)\n",
    "        feat_chunks.append(feat_batch)\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    X_tcr_all = np.vstack(tcr_chunks)\n",
    "    X_epi_all = np.vstack(epi_chunks)\n",
    "    X_feat_all = np.vstack(feat_chunks)\n",
    "\n",
    "    is_binding = preds >= threshold\n",
    "    is_nonbinding = ~is_binding\n",
    "\n",
    "    return (\n",
    "        X_tcr_all[is_binding], X_epi_all[is_binding], X_feat_all[is_binding],\n",
    "        X_tcr_all[is_nonbinding], X_epi_all[is_nonbinding], X_feat_all[is_nonbinding]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16277de4-d274-4b7b-80c4-adc2f6191693",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = NUM_SAMPLES\n",
    "\n",
    "X_batch_tcr = X_test_tcr[:n_samples]\n",
    "X_batch_epi = X_test_epitope[:n_samples]\n",
    "X_batch_feat = X_test_features[:n_samples]\n",
    "\n",
    "X_tcr_bind, X_epi_bind, X_feat_bind, X_tcr_non, X_epi_non, X_feat_non = split_by_prediction(\n",
    "    model, X_batch_tcr, X_batch_epi, X_batch_feat, batch_size=128\n",
    ")\n",
    "\n",
    "mean_sal_tcr_bind, mean_sal_epi_bind = compute_saliency_over_dataset(model, X_tcr_bind, X_epi_bind, X_feat_bind, batch_size=128)\n",
    "mean_sal_tcr_non, mean_sal_epi_non = compute_saliency_over_dataset(model, X_tcr_non, X_epi_non, X_feat_non, batch_size=128)\n",
    "\n",
    "def plot_saliency_comparison(sal_class0, sal_class1, title, labels):\n",
    "    x = np.arange(len(sal_class0))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.bar(x - width/2, sal_class0, width, label='Non-binding', color='steelblue')\n",
    "    plt.bar(x + width/2, sal_class1, width, label='Binding', color='darkorange')\n",
    "    plt.xlabel(\"Position\")\n",
    "    plt.ylabel(\"Mean Saliency\")\n",
    "    #plt.title(title)\n",
    "    plt.xticks(ticks=x, labels=labels)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{path}{title}_{NUM_SAMPLES}.png', dpi=96, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_saliency_comparison(mean_sal_tcr_non, mean_sal_tcr_bind, \"TCR Saliency by Predicted Class\", labels=range(26))\n",
    "\n",
    "plot_saliency_comparison(mean_sal_epi_non, mean_sal_epi_bind, \"Epitope Saliency by Predicted Class\", labels=range(24))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96475259-b4dc-45a3-8be0-20f5ac11fd7c",
   "metadata": {},
   "source": [
    "### Analyze true vs predicted labels instead (instead of predicted-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c0865f-9971-42d8-ab07-7aaa3573ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_by_true_label(model, X_tcr, X_epi, X_feat, y_true, batch_size=128):\n",
    "    y_true = np.array(y_true).reshape(-1).astype(int)\n",
    "\n",
    "    saliency_tcr_bind = []\n",
    "    saliency_epi_bind = []\n",
    "    saliency_tcr_non = []\n",
    "    saliency_epi_non = []\n",
    "\n",
    "    num_samples = len(y_true)\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        tcr_batch = np.array(X_tcr[i:i + batch_size])\n",
    "        epi_batch = np.array(X_epi[i:i + batch_size])\n",
    "        feat_batch = np.array(X_feat[i:i + batch_size])\n",
    "        y_batch = y_true[i:i + batch_size]\n",
    "\n",
    "        is_bind = y_batch == 1\n",
    "        is_non = y_batch == 0\n",
    "\n",
    "        if np.any(is_bind):\n",
    "            sal_tcr, sal_epi = compute_batch_saliency(\n",
    "                model, tcr_batch[is_bind], epi_batch[is_bind], feat_batch[is_bind]\n",
    "            )\n",
    "            saliency_tcr_bind.append(sal_tcr)\n",
    "            saliency_epi_bind.append(sal_epi)\n",
    "\n",
    "        if np.any(is_non):\n",
    "            sal_tcr, sal_epi = compute_batch_saliency(\n",
    "                model, tcr_batch[is_non], epi_batch[is_non], feat_batch[is_non]\n",
    "            )\n",
    "            saliency_tcr_non.append(sal_tcr)\n",
    "            saliency_epi_non.append(sal_epi)\n",
    "\n",
    "    def safe_mean(stack_list, expected_dim):\n",
    "        if stack_list:\n",
    "            return np.mean(np.vstack(stack_list), axis=0)\n",
    "        else:\n",
    "            return np.zeros(expected_dim)\n",
    "\n",
    "    tcr_len = X_tcr.shape[1]\n",
    "    epi_len = X_epi.shape[1]\n",
    "\n",
    "    mean_sal_tcr_bind = safe_mean(saliency_tcr_bind, expected_dim=tcr_len)\n",
    "    mean_sal_epi_bind = safe_mean(saliency_epi_bind, expected_dim=epi_len)\n",
    "    mean_sal_tcr_non = safe_mean(saliency_tcr_non, expected_dim=tcr_len)\n",
    "    mean_sal_epi_non = safe_mean(saliency_epi_non, expected_dim=epi_len)\n",
    "\n",
    "    return mean_sal_tcr_bind, mean_sal_epi_bind, mean_sal_tcr_non, mean_sal_epi_non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7ff1d-181b-4fd7-b16c-aaed76d0050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = NUM_SAMPLES\n",
    "X_tcr_batch = X_test_tcr[:n_samples]\n",
    "X_epi_batch = X_test_epitope[:n_samples]\n",
    "X_feat_batch = X_test_features[:n_samples]\n",
    "y_true_batch = y_test_labels[:n_samples]\n",
    "\n",
    "mean_sal_tcr_bind, mean_sal_epi_bind, mean_sal_tcr_non, mean_sal_epi_non = compute_saliency_by_true_label(\n",
    "    model, X_tcr_batch, X_epi_batch, X_feat_batch, y_true_batch, batch_size=128\n",
    ")\n",
    "\n",
    "def plot_saliency_comparison(sal_class0, sal_class1, title, labels):\n",
    "    x = np.arange(len(sal_class0))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.bar(x - width/2, sal_class0, width, label='True Non-binding', color='steelblue')\n",
    "    plt.bar(x + width/2, sal_class1, width, label='True Binding', color='darkorange')\n",
    "    plt.xlabel(\"Position\")\n",
    "    plt.ylabel(\"Mean Saliency\")\n",
    "    plt.xticks(ticks=x, labels=labels)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{path}{title}_{NUM_SAMPLES}.png', dpi=96, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_saliency_comparison(mean_sal_tcr_non, mean_sal_tcr_bind, \"TCR Saliency by True Label\", labels=range(26))\n",
    "plot_saliency_comparison(mean_sal_epi_non, mean_sal_epi_bind, \"Epitope Saliency by True Label\", labels=range(24))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8842c5-49ad-4c6c-aed8-0e890abe9809",
   "metadata": {},
   "source": [
    "#### SHAP on TCR & Epitope Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9dc235-0ff1-49be-8433-285378d3868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "X_tcr_test = X_test_tcr[:NUM_SAMPLES]\n",
    "X_epi_test = X_test_epitope[:NUM_SAMPLES]\n",
    "X_feat_test = X_test_features[:NUM_SAMPLES]\n",
    "\n",
    "n_background = NUM_SAMPLES//2\n",
    "\n",
    "X_tcr_bg = X_positive_tcr[:n_background]\n",
    "X_epi_bg = X_positive_epitope[:n_background]\n",
    "X_feat_bg = X_positive_features[:n_background]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f79aa8-6492-4cfa-8b95-3cca046143b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.GradientExplainer(\n",
    "    model,\n",
    "    data=[\n",
    "        tf.convert_to_tensor(X_tcr_bg),\n",
    "        tf.convert_to_tensor(X_epi_bg),\n",
    "        tf.convert_to_tensor(X_feat_bg)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d756fb6a-005c-4adb-9e48-8816bdcd6d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tcr_test = X_tcr_test.numpy() if isinstance(X_tcr_test, tf.Tensor) else X_tcr_test\n",
    "X_epi_test = X_epi_test.numpy() if isinstance(X_epi_test, tf.Tensor) else X_epi_test\n",
    "X_feat_test = X_feat_test.numpy() if isinstance(X_feat_test, tf.Tensor) else X_feat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574ba11e-8a44-4d54-b493-e50a53176e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values([\n",
    "    X_tcr_test,\n",
    "    X_epi_test,\n",
    "    X_feat_test\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8960a4-9155-474e-8df5-aaa96a0aaff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_shap_position_importance(shap_input, title):\n",
    "    shap_input = np.squeeze(shap_input)\n",
    "    mean_abs_importance = np.mean(np.abs(shap_input), axis=(0, 2))\n",
    "\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.bar(range(len(mean_abs_importance)), mean_abs_importance)\n",
    "    plt.xlabel(\"Position\")\n",
    "    plt.ylabel(\"Mean |SHAP value|\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{path}{title}_{NUM_SAMPLES}.png', dpi=96, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_shap_position_importance(shap_values[0], \"TCR Embedding Importance (SHAP)\")\n",
    "plot_shap_position_importance(shap_values[1], \"Epitope Embedding Importance (SHAP)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae04967-0b9b-49b2-b7d6-b11f5f177a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 50000\n",
    "\n",
    "X_tcr_test = X_test_tcr[:NUM_SAMPLES]\n",
    "X_epi_test = X_test_epitope[:NUM_SAMPLES]\n",
    "X_feat_test = X_test_features[:NUM_SAMPLES]\n",
    "\n",
    "n_background = NUM_SAMPLES // 2\n",
    "X_tcr_bg = X_positive_tcr[:n_background]\n",
    "X_epi_bg = X_positive_epitope[:n_background]\n",
    "X_feat_bg = X_positive_features[:n_background]\n",
    "\n",
    "explainer = shap.GradientExplainer(\n",
    "    model,\n",
    "    data=[\n",
    "        tf.convert_to_tensor(X_tcr_bg),\n",
    "        tf.convert_to_tensor(X_epi_bg),\n",
    "        tf.convert_to_tensor(X_feat_bg)\n",
    "    ]\n",
    ")\n",
    "\n",
    "def compute_shap_values_in_batches(explainer, X_tcr, X_epi, X_feat, batch_size=128):\n",
    "    shap_vals_tcr = []\n",
    "    shap_vals_epi = []\n",
    "    shap_vals_feat = []\n",
    "\n",
    "    for i in range(0, len(X_tcr), batch_size):\n",
    "        tcr_batch = X_tcr[i:i+batch_size]\n",
    "        epi_batch = X_epi[i:i+batch_size]\n",
    "        feat_batch = X_feat[i:i+batch_size]\n",
    "\n",
    "        shap_batch = explainer.shap_values([\n",
    "            tcr_batch,\n",
    "            epi_batch,\n",
    "            feat_batch\n",
    "        ])\n",
    "\n",
    "        shap_vals_tcr.append(shap_batch[0])\n",
    "        shap_vals_epi.append(shap_batch[1])\n",
    "        shap_vals_feat.append(shap_batch[2])\n",
    "\n",
    "    shap_vals_tcr = np.concatenate(shap_vals_tcr, axis=0)\n",
    "    shap_vals_epi = np.concatenate(shap_vals_epi, axis=0)\n",
    "    shap_vals_feat = np.concatenate(shap_vals_feat, axis=0)\n",
    "\n",
    "    return shap_vals_tcr, shap_vals_epi, shap_vals_feat\n",
    "\n",
    "X_tcr_test = X_tcr_test.numpy() if isinstance(X_tcr_test, tf.Tensor) else X_tcr_test\n",
    "X_epi_test = X_epi_test.numpy() if isinstance(X_epi_test, tf.Tensor) else X_epi_test\n",
    "X_feat_test = X_feat_test.numpy() if isinstance(X_feat_test, tf.Tensor) else X_feat_test\n",
    "\n",
    "shap_tcr, shap_epi, shap_feat = compute_shap_values_in_batches(\n",
    "    explainer,\n",
    "    X_tcr_test,\n",
    "    X_epi_test,\n",
    "    X_feat_test,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2581a938-c06b-46d1-991c-c39e611d6f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_position_importance(shap_input, title):\n",
    "    shap_input = np.squeeze(shap_input)\n",
    "    mean_abs_importance = np.mean(np.abs(shap_input), axis=(0, 2))\n",
    "\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.bar(range(len(mean_abs_importance)), mean_abs_importance)\n",
    "    plt.xlabel(\"Position\")\n",
    "    plt.ylabel(\"Mean |SHAP value|\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{path}{title}_{NUM_SAMPLES}.png', dpi=96, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "plot_shap_position_importance(shap_tcr, \"TCR Embedding Importance (SHAP)\")\n",
    "plot_shap_position_importance(shap_epi, \"Epitope Embedding Importance (SHAP)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2a6c37-562b-47a6-a039-abe4eedab379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
